{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: catboost in ./.rent_venv/lib/python3.8/site-packages (1.2.8)\n",
      "Requirement already satisfied: graphviz in ./.rent_venv/lib/python3.8/site-packages (from catboost) (0.20.3)\n",
      "Requirement already satisfied: matplotlib in ./.rent_venv/lib/python3.8/site-packages (from catboost) (3.7.5)\n",
      "Requirement already satisfied: numpy<3.0,>=1.16.0 in ./.rent_venv/lib/python3.8/site-packages (from catboost) (1.24.4)\n",
      "Requirement already satisfied: pandas>=0.24 in ./.rent_venv/lib/python3.8/site-packages (from catboost) (2.0.3)\n",
      "Requirement already satisfied: scipy in ./.rent_venv/lib/python3.8/site-packages (from catboost) (1.10.1)\n",
      "Requirement already satisfied: plotly in ./.rent_venv/lib/python3.8/site-packages (from catboost) (6.1.0)\n",
      "Requirement already satisfied: six in ./.rent_venv/lib/python3.8/site-packages (from catboost) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.rent_venv/lib/python3.8/site-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.rent_venv/lib/python3.8/site-packages (from pandas>=0.24->catboost) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./.rent_venv/lib/python3.8/site-packages (from pandas>=0.24->catboost) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.rent_venv/lib/python3.8/site-packages (from matplotlib->catboost) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.rent_venv/lib/python3.8/site-packages (from matplotlib->catboost) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.rent_venv/lib/python3.8/site-packages (from matplotlib->catboost) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./.rent_venv/lib/python3.8/site-packages (from matplotlib->catboost) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.rent_venv/lib/python3.8/site-packages (from matplotlib->catboost) (25.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./.rent_venv/lib/python3.8/site-packages (from matplotlib->catboost) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.rent_venv/lib/python3.8/site-packages (from matplotlib->catboost) (3.1.4)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in ./.rent_venv/lib/python3.8/site-packages (from matplotlib->catboost) (6.4.5)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in ./.rent_venv/lib/python3.8/site-packages (from plotly->catboost) (1.40.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./.rent_venv/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib->catboost) (3.20.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_regression\n",
    "from typing import Tuple, List\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "%pip install catboost\n",
    "import catboost as cb\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm.auto import tqdm # For progress bars\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 69\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv('/content/drive/MyDrive/Education/University/Master/ML/tr_clean.csv')\n",
    "# test_df = pd.read_csv('/content/drive/MyDrive/Education/University/Master/ML/te_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('tr_clean.csv')\n",
    "test_df = pd.read_csv('te_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns of type object:\n",
      "Index(['description', 'zone'], dtype='object')\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4494 entries, 0 to 4493\n",
      "Columns: 109 entries, price to sqm_x_high_efficiency\n",
      "dtypes: float64(48), int64(61)\n",
      "memory usage: 3.7 MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2833 entries, 0 to 2832\n",
      "Columns: 108 entries, square_meters to sqm_x_high_efficiency\n",
      "dtypes: float64(47), int64(61)\n",
      "memory usage: 2.3 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Get columns of type object\n",
    "object_cols = train_df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Print object columns\n",
    "print(\"Columns of type object:\")\n",
    "print(object_cols)\n",
    "\n",
    "# Drop object columns\n",
    "train_df = train_df.drop(columns=object_cols)\n",
    "test_df = test_df.drop(columns=object_cols)\n",
    "\n",
    "print(train_df.info())\n",
    "print(test_df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_correlations(X: pd.DataFrame, \n",
    "                         y: pd.Series, \n",
    "                         n_features: int = 30,\n",
    "                         figsize: tuple = (12, 10),\n",
    "                         cmap: str = 'coolwarm',\n",
    "                         mask_diagonal: bool = True):\n",
    "    \"\"\"\n",
    "    Create and plot a correlation heatmap for the top n most correlated features.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : pandas DataFrame\n",
    "        Feature matrix\n",
    "    y : pandas Series\n",
    "        Target variable\n",
    "    n_features : int, default=30\n",
    "        Number of top correlated features to include in the plot\n",
    "    figsize : tuple, default=(12, 10)\n",
    "        Figure size for the plot\n",
    "    cmap : str, default='coolwarm'\n",
    "        Color map for the heatmap\n",
    "    mask_diagonal : bool, default=True\n",
    "        Whether to mask the diagonal of the correlation matrix\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    fig : matplotlib Figure\n",
    "        The generated figure\n",
    "    top_corr_df : pandas DataFrame\n",
    "        DataFrame containing the correlation values for the top features\n",
    "    \"\"\"\n",
    "    # Combine features and target\n",
    "    data = pd.concat([X, y], axis=1)\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = data.corr()\n",
    "    \n",
    "    # Get average absolute correlation for each feature\n",
    "    avg_corr = corr_matrix.abs().mean().sort_values(ascending=False)\n",
    "    \n",
    "    # Select top n features (excluding the target if it's in the top)\n",
    "    top_features = avg_corr.head(n_features + 1)  # +1 in case target is in top\n",
    "    if y.name in top_features.index:\n",
    "        top_features = avg_corr[avg_corr.index != y.name].head(n_features)\n",
    "    else:\n",
    "        top_features = top_features.head(n_features)\n",
    "    \n",
    "    # Add target variable to the list of features\n",
    "    selected_features = list(top_features.index) + [y.name]\n",
    "    \n",
    "    # Create correlation matrix for selected features\n",
    "    top_corr = corr_matrix.loc[selected_features, selected_features]\n",
    "    \n",
    "    # Create mask for diagonal if requested\n",
    "    if mask_diagonal:\n",
    "        mask = np.eye(len(selected_features), dtype=bool)\n",
    "    else:\n",
    "        mask = None\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(top_corr, \n",
    "                mask=mask,\n",
    "                cmap=cmap,\n",
    "                center=0,\n",
    "                annot=True,\n",
    "                fmt='.2f',\n",
    "                square=True,\n",
    "                linewidths=0.5,\n",
    "                cbar_kws={'shrink': .5},\n",
    "                ax=ax)\n",
    "    \n",
    "    # Rotate x-axis labels\n",
    "    plt.xticks(rotation=70, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    # Add title\n",
    "    plt.title(f'Correlation Heatmap: Top {n_features} Most Correlated Features\\n',\n",
    "              pad=20,\n",
    "              fontsize=30)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Create summary of correlations with target\n",
    "    target_corr = top_corr[y.name].sort_values(ascending=False)\n",
    "    print(\"\\nTop 20 Features Most Correlated with Target:\")\n",
    "    print(target_corr[target_corr.index != y.name].head(20))\n",
    "    \n",
    "    # Print features with highest average correlation\n",
    "    print(\"\\nFeatures with Highest Average Absolute Correlation:\")\n",
    "    print(avg_corr[avg_corr.index != y.name].head(10))\n",
    "    \n",
    "    return fig, top_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop(columns=['price'])\n",
    "y_train = train_df['price']\n",
    "\n",
    "# fig, corr_matrix = plot_top_correlations(\n",
    "#     X=X_train,\n",
    "#     y=y_train,\n",
    "#     n_features=50,\n",
    "#     figsize=(20, 20),\n",
    "#     cmap='coolwarm',\n",
    "#     mask_diagonal=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_distributions(df):\n",
    "    # looking to apply tranformations to narmalise\n",
    "    plt.figure(figsize=(20, 100))\n",
    "\n",
    "    # Calculate number of rows and columns for subplots\n",
    "    n_features = len(train_df.columns)\n",
    "    n_cols = 3\n",
    "    n_rows = (n_features + n_cols - 1) // n_cols\n",
    "\n",
    "    # Create subplots for each feature\n",
    "    for idx, feature in enumerate(train_df.columns, 1):\n",
    "        plt.subplot(n_rows, n_cols, idx)\n",
    "        \n",
    "        # Create histogram with KDE\n",
    "        sns.histplot(data=train_df, x=feature, kde=True)\n",
    "        plt.title(f'Distribution of {feature}')\n",
    "        plt.xlabel(feature)\n",
    "        plt.ylabel('Count')\n",
    "        \n",
    "    plt.suptitle('Feature distributions in Train df')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# plot_feature_distributions(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_transformations(df, variables, plot=False):\n",
    "    \"\"\"\n",
    "    Apply np.log1p transformation to specified variables and plot before vs after distributions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        Input DataFrame containing the variables to transform\n",
    "    variables : list\n",
    "        List of column names to transform and plot\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    transformed_df : pandas DataFrame\n",
    "        DataFrame with added log-transformed columns\n",
    "    \"\"\"\n",
    "    # Create a copy of the dataframe\n",
    "    transformed_df = df.copy()\n",
    "    \n",
    "    # Set up the plotting area\n",
    "    n_vars = len(variables)\n",
    "    if plot:\n",
    "        fig, axes = plt.subplots(n_vars, 2, figsize=(15, 4*n_vars))\n",
    "        fig.suptitle('Original vs Log-Transformed Distributions', y=1.02, fontsize=16)\n",
    "    \n",
    "    # For each variable\n",
    "    for idx, var in enumerate(variables):\n",
    "        if var not in df.columns:\n",
    "            continue\n",
    "        # Create transformed column name\n",
    "        log_var = f'log_{var}'\n",
    "        \n",
    "        # Apply log1p transformation\n",
    "        transformed_df[log_var] = np.log1p(df[var])\n",
    "        transformed_df = transformed_df.drop(columns=[var])\n",
    "\n",
    "        if var == 'price':\n",
    "            transformed_df = transformed_df.rename(columns={log_var: 'y'})\n",
    "        \n",
    "        if plot:\n",
    "            # Plot original distribution\n",
    "            sns.histplot(data=df, x=var, ax=axes[idx, 0], kde=True)\n",
    "            axes[idx, 0].set_title(f'Original: {var}')\n",
    "            axes[idx, 0].set_xlabel(var)\n",
    "            \n",
    "            # Plot transformed distribution\n",
    "            sns.histplot(data=transformed_df, x=log_var, ax=axes[idx, 1], kde=True)\n",
    "            axes[idx, 1].set_title(f'Log-Transformed: {var}')\n",
    "            axes[idx, 1].set_xlabel(f'log1p({var})')\n",
    "    \n",
    "    if plot:\n",
    "        plt.tight_layout()\n",
    "    \n",
    "    return transformed_df\n",
    "\n",
    "# Variables to transform\n",
    "variables_to_transform = [\n",
    "    'square_meters',\n",
    "    'sqm_x_zone_rank',\n",
    "    'sqm_x_condition',\n",
    "    'bedrooms_x_zone_rank'\n",
    "]\n",
    "\n",
    "train_df = log_transformations(train_df, variables_to_transform)\n",
    "test_df = log_transformations(test_df, variables_to_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate target variable before scaling\n",
    "y_train = train_df['y']\n",
    "X_train = train_df.drop('y', axis=1)\n",
    "\n",
    "# Scale features\n",
    "feature_scaler = MinMaxScaler()\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    feature_scaler.fit_transform(X_train),\n",
    "    columns=X_train.columns,\n",
    "    index=X_train.index\n",
    ")\n",
    "\n",
    "# Scale test features - ensure columns match training data\n",
    "test_df_scaled = pd.DataFrame(\n",
    "    feature_scaler.transform(test_df[X_train.columns]),\n",
    "    columns=X_train.columns,\n",
    "    index=test_df.index\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree-Based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCORING = 'neg_mean_absolute_error' # For RandomizedSearchCV/GridSearchCV\n",
    "\n",
    "def evaluate_model(y_true, y_pred, model_name=\"Model\"):\n",
    "    \"\"\"Calculates and prints regression metrics.\"\"\"\n",
    "    y_true_eval = np.array(y_true).copy() \n",
    "    y_pred_eval = np.array(y_pred).copy()\n",
    "\n",
    "    mse = mean_squared_error(y_true_eval, y_pred_eval)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true_eval, y_pred_eval)\n",
    "    r2 = r2_score(y_true_eval, y_pred_eval)\n",
    "    \n",
    "    print(f\"--- {model_name} Performance ---\")\n",
    "    print(f\"MAE (Primary): {mae:.4f}\")\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"R2 Score: {r2:.4f}\")\n",
    "    print(\"----------------------------- ulcers\")\n",
    "    return {'mse': mse, 'rmse': rmse, 'mae': mae, 'r2': r2}\n",
    "\n",
    "N_SPLITS = 5\n",
    "cv_strategy = KFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original full train data shape: (4494, 108)\n",
      "New X_train shape: (4089, 108), new y_train shape: (4089,)\n",
      "X_dev shape: (405, 108), y_dev shape: (405,)\n",
      "Data loaded, prepared, and split into train/dev sets.\n"
     ]
    }
   ],
   "source": [
    "X_train_data = train_df.drop(columns=['y'])\n",
    "y_train_data = train_df['y']\n",
    "\n",
    "X_test_data = test_df\n",
    "\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(\n",
    "    X_train_data, \n",
    "    y_train_data, \n",
    "    test_size=0.09, \n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f\"Original full train data shape: {X_train_data.shape}\")\n",
    "print(f\"New X_train shape: {X_train.shape}, new y_train shape: {y_train.shape}\")\n",
    "print(f\"X_dev shape: {X_dev.shape}, y_dev shape: {y_dev.shape}\")\n",
    "print(\"Data loaded, prepared, and split into train/dev sets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RandomForest...\n",
      "--- Baseline RandomForest Performance ---\n",
      "MSE: 0.0054\n",
      "RMSE: 0.0737\n",
      "MAE: 0.0513\n",
      "R2 Score: 0.8113\n",
      "-----------------------------\n",
      "Training XGBoost...\n",
      "--- Baseline XGBoost Performance ---\n",
      "MSE: 0.0057\n",
      "RMSE: 0.0756\n",
      "MAE: 0.0531\n",
      "R2 Score: 0.8014\n",
      "-----------------------------\n",
      "Training LightGBM...\n",
      "--- Baseline LightGBM Performance ---\n",
      "MSE: 0.0055\n",
      "RMSE: 0.0740\n",
      "MAE: 0.0517\n",
      "R2 Score: 0.8096\n",
      "-----------------------------\n",
      "Training CatBoost...\n",
      "--- Baseline CatBoost Performance ---\n",
      "MSE: 0.0055\n",
      "RMSE: 0.0742\n",
      "MAE: 0.0518\n",
      "R2 Score: 0.8088\n",
      "-----------------------------\n",
      "\n",
      "Baseline Model Training Complete.\n",
      "\n",
      "Summary of Baseline Results (on Dev Set):\n",
      "--------------------------------------------------\n",
      "\n",
      "RandomForest:\n",
      "  MSE:  0.0054\n",
      "  RMSE: 0.0737\n",
      "  MAE:  0.0513\n",
      "  R2:   0.8113\n",
      "\n",
      "XGBoost:\n",
      "  MSE:  0.0057\n",
      "  RMSE: 0.0756\n",
      "  MAE:  0.0531\n",
      "  R2:   0.8014\n",
      "\n",
      "LightGBM:\n",
      "  MSE:  0.0055\n",
      "  RMSE: 0.0740\n",
      "  MAE:  0.0517\n",
      "  R2:   0.8096\n",
      "\n",
      "CatBoost:\n",
      "  MSE:  0.0055\n",
      "  RMSE: 0.0742\n",
      "  MAE:  0.0518\n",
      "  R2:   0.8088\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "baseline_models = {\n",
    "    \"RandomForest\": RandomForestRegressor(random_state=RANDOM_SEED, n_jobs=-1),\n",
    "    \"XGBoost\": xgb.XGBRegressor(random_state=RANDOM_SEED, n_jobs=-1),\n",
    "    \"LightGBM\": lgb.LGBMRegressor(random_state=RANDOM_SEED, n_jobs=-1, verbosity=-1),\n",
    "    \"CatBoost\": cb.CatBoostRegressor(random_state=RANDOM_SEED, verbose=0)\n",
    "}\n",
    "\n",
    "baseline_results = {}\n",
    "do_baseline = False\n",
    "\n",
    "if do_baseline:\n",
    "    for name, model in baseline_models.items():\n",
    "        print(f\"Training {name}...\")\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred_dev = model.predict(X_dev) # Evaluate on the dev set\n",
    "        baseline_results[name] = evaluate_model(y_dev, y_pred_dev, \n",
    "                                                model_name=f\"Baseline {name}\",\n",
    "                                                target_transformer=target_scaler)\n",
    "\n",
    "    print(\"\\nBaseline Model Training Complete.\")\n",
    "    print(\"\\nSummary of Baseline Results (on Dev Set):\")\n",
    "    print(\"-\" * 50)\n",
    "    for model_name, metrics in baseline_results.items():\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(f\"  MSE:  {metrics['mse']:.4f}\")\n",
    "        print(f\"  RMSE: {metrics['rmse']:.4f}\")\n",
    "        print(f\"  MAE:  {metrics['mae']:.4f}\")\n",
    "        print(f\"  R2:   {metrics['r2']:.4f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ITER_RANDOM_SEARCH = 50 # Number of parameter settings that are sampled for RandomizedSearchCV.\n",
    "\n",
    "tuned_models = {}\n",
    "best_params_all_models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best RandomForest Params: {'n_estimators': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 0.5, 'max_depth': 30, 'bootstrap': True}\n",
      "Best RandomForest Score (CV neg_mean_squared_error): -0.0050\n"
     ]
    }
   ],
   "source": [
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2', 0.5, 0.7, 1.0],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "rf_random_search = RandomizedSearchCV(\n",
    "    RandomForestRegressor(random_state=RANDOM_SEED, n_jobs=-1),\n",
    "    param_distributions=rf_param_grid,\n",
    "    n_iter=N_ITER_RANDOM_SEARCH,\n",
    "    cv=cv_strategy,\n",
    "    scoring=SCORING,\n",
    "    random_state=RANDOM_SEED,\n",
    "    n_jobs=-1, # Use all available cores\n",
    "    verbose=1\n",
    ")\n",
    "rf_random_search.fit(X_train, y_train)\n",
    "tuned_models[\"RandomForest\"] = rf_random_search.best_estimator_\n",
    "best_params_all_models[\"RandomForest\"] = rf_random_search.best_params_\n",
    "print(f\"Best RandomForest Params: {rf_random_search.best_params_}\")\n",
    "print(f\"Best RandomForest Score (CV {SCORING}, on transformed y): {rf_random_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_param_grid = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2]\n",
    "}\n",
    "xgb_random_search = RandomizedSearchCV(\n",
    "    xgb.XGBRegressor(random_state=RANDOM_SEED, n_jobs=-1, objective='reg:absoluteerror'), # Explicitly set objective\n",
    "    param_distributions=xgb_param_grid,\n",
    "    n_iter=N_ITER_RANDOM_SEARCH,\n",
    "    cv=cv_strategy,\n",
    "    scoring=SCORING,\n",
    "    random_state=RANDOM_SEED,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "xgb_random_search.fit(X_train, y_train)\n",
    "tuned_models[\"XGBoost\"] = xgb_random_search.best_estimator_\n",
    "best_params_all_models[\"XGBoost\"] = xgb_random_search.best_params_\n",
    "print(f\"Best XGBoost Params: {xgb_random_search.best_params_}\")\n",
    "print(f\"Best XGBoost Score (CV {SCORING}, on transformed y): {xgb_random_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_param_grid = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'iterations': [100, 200, 300, 500], # n_estimators for CatBoost\n",
    "    'depth': [4, 6, 8, 10], # max_depth\n",
    "    'l2_leaf_reg': [1, 3, 5, 7], # L2 regularization\n",
    "    'rsm': [0.8, 0.9, 1.0],  # colsample_bylevel (analogous to colsample_bytree)\n",
    "    'subsample': [0.8, 0.9, 1.0], # for CatBoost, this is only effective if bootstrap_type is 'Bernoulli' or 'Poisson'\n",
    "    'min_data_in_leaf': [1, 10, 20] # min_child_samples\n",
    "}\n",
    "cat_random_search = RandomizedSearchCV(\n",
    "    cb.CatBoostRegressor(random_state=RANDOM_SEED, verbose=0, allow_writing_files=False),\n",
    "    param_distributions=cat_param_grid,\n",
    "    n_iter=N_ITER_RANDOM_SEARCH,\n",
    "    cv=cv_strategy,\n",
    "    scoring=SCORING,\n",
    "    random_state=RANDOM_SEED,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "cat_random_search.fit(X_train, y_train)\n",
    "tuned_models[\"CatBoost\"] = cat_random_search.best_estimator_\n",
    "best_params_all_models[\"CatBoost\"] = cat_random_search.best_params_\n",
    "print(f\"Best CatBoost Params: {cat_random_search.best_params_}\")\n",
    "print(f\"Best CatBoost Score (CV {SCORING}, on transformed y): {cat_random_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate on Dev Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_results_dev = {} # Renamed from tuned_results_test\n",
    "for name, model in tuned_models.items():\n",
    "    print(f\"Evaluating Tuned {name}...\")\n",
    "    y_pred_dev = model.predict(X_dev) # Evaluate on the dev set\n",
    "    tuned_results_dev[name] = evaluate_model(y_dev, y_pred_dev,\n",
    "                                            model_name=f\"Tuned {name}\",\n",
    "                                            target_transformer=target_scaler)\n",
    "\n",
    "print(\"\\nTuned Model Evaluation on Dev Set Complete.\")\n",
    "print(\"Tuned Model Dev Set Results:\", tuned_results_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = []\n",
    "for name, model in tuned_models.items():\n",
    "    if model is not None: # Check if model was successfully tuned\n",
    "        estimators.append((name.lower(), model)) # Naming convention for VotingRegressor\n",
    "\n",
    "if estimators:\n",
    "    voting_regressor = VotingRegressor(estimators=estimators, n_jobs=-1)\n",
    "\n",
    "    print(\"Training Voting Regressor...\")\n",
    "    voting_regressor.fit(X_train, y_train)\n",
    "    print(\"Evaluating Voting Regressor on Dev Set...\")\n",
    "    y_pred_ensemble_dev = voting_regressor.predict(X_dev) # Evaluate on the dev set\n",
    "    ensemble_results_dev = evaluate_model(y_dev, y_pred_ensemble_dev, \n",
    "                                          model_name=\"Voting Ensemble\",\n",
    "                                          target_transformer=target_scaler)\n",
    "    print(\"Voting Ensemble Dev Set Results:\", ensemble_results_dev)\n",
    "else:\n",
    "    print(\"No tuned models available to create an ensemble.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- FINAL MODEL PERFORMANCE SUMMARY (Dev Set, metrics in original scale) ---\")\n",
    "\n",
    "print(\"\\nBaseline Models (on Dev Set):\")\n",
    "for name, metrics in baseline_results.items():\n",
    "    print(f\"  {name}: MAE={metrics['mae']:.4f}, R2={metrics['r2']:.4f}\")\n",
    "\n",
    "print(\"\\nTuned Individual Models (on Dev Set):\")\n",
    "for name, metrics in tuned_results_dev.items():\n",
    "    print(f\"  {name}: MAE={metrics['mae']:.4f}, R2={metrics['r2']:.4f}\")\n",
    "\n",
    "if 'ensemble_results_dev' in locals() and ensemble_results_dev:\n",
    "    print(\"\\nEnsemble Model (Voting Regressor on Dev Set):\")\n",
    "    print(f\"  Voting Ensemble: MAE={ensemble_results_dev['mae']:.4f}, R2={ensemble_results_dev['r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory if it doesn't exist\n",
    "models_dir = 'models'\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "    print(f\"Created directory: {models_dir}\")\n",
    "\n",
    "# Save best individual models\n",
    "for name, model in tuned_models.items():\n",
    "    if model is not None:\n",
    "        model_path = os.path.join(models_dir, f'best_{name.lower()}_model.joblib')\n",
    "        joblib.dump(model, model_path)\n",
    "        print(f\"Saved {name} model to: {model_path}\")\n",
    "\n",
    "# Save ensemble model if it exists\n",
    "if 'voting_regressor' in locals() and voting_regressor is not None:\n",
    "    ensemble_path = os.path.join(models_dir, 'best_ensemble_model.joblib')\n",
    "    joblib.dump(voting_regressor, ensemble_path)\n",
    "    print(f\"Saved Ensemble model to: {ensemble_path}\")\n",
    "\n",
    "# Save best parameters for each model\n",
    "best_params_path = os.path.join(models_dir, 'best_parameters.json')\n",
    "pd.Series(best_params_all_models).to_json(best_params_path)\n",
    "print(f\"Saved best parameters to: {best_params_path}\")\n",
    "\n",
    "# Save feature scaler if it exists (assuming it's defined in your data preprocessing)\n",
    "if 'feature_scaler' in locals() and feature_scaler is not None:\n",
    "    scaler_path = os.path.join(models_dir, 'feature_scaler.joblib')\n",
    "    joblib.dump(feature_scaler, scaler_path)\n",
    "    print(f\"Saved feature scaler to: {scaler_path}\")\n",
    "\n",
    "# Save target transformer if it exists (assuming it's defined in your data preprocessing)\n",
    "if 'target_scaler' in locals() and target_scaler is not None:\n",
    "    target_scaler_path = os.path.join(models_dir, 'target_scaler.joblib')\n",
    "    joblib.dump(target_scaler, target_scaler_path)\n",
    "    print(f\"Saved target scaler to: {target_scaler_path}\")\n",
    "\n",
    "print(\"\\nAll models and parameters have been saved successfully.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Analyze feature importances for the best model(s)\n",
    "2. Perform GridSearchCV for more fine-grained tuning around the best parameters found by RandomizedSearchCV (optimizing for MAE)\n",
    "3. Experiment with different ensemble weighting strategies, potentially based on MAE performance\n",
    "4. If you obtain a separate, final TEST set (with labels), evaluate the chosen model on it for a final unbiased performance assessment\n",
    "5. Analyze error patterns (e.g., residuals plot) for the best model on the dev set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN-based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "PyTorch Version: 2.2.2\n"
     ]
    }
   ],
   "source": [
    "PRIMARY_METRIC = 'mae' # Mean Absolute Error\n",
    "LOSS_FUNCTION_NAME = 'mean_absolute_error' # For consistency, actual loss is nn.L1Loss\n",
    "\n",
    "# Setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def evaluate_nn_model(y_true, y_pred, model_name=\"NN Model\", target_transformer=None):\n",
    "    \"\"\"Calculates and prints regression metrics for NN.\n",
    "    If target_transformer is provided, it will inverse transform y_true and y_pred \n",
    "    before calculating metrics to report them in the original scale.\n",
    "    Assumes y_true and y_pred are in the log1p+scaled domain if transformer is provided.\n",
    "    \"\"\"\n",
    "    y_true_eval_np = y_true.cpu().numpy() if isinstance(y_true, torch.Tensor) else np.array(y_true).copy()\n",
    "    y_pred_eval_np = y_pred.cpu().numpy() if isinstance(y_pred, torch.Tensor) else np.array(y_pred).copy()\n",
    "\n",
    "    if target_transformer is not None:\n",
    "        if y_true_eval_np.ndim == 1:\n",
    "            y_true_eval_np = y_true_eval_np.reshape(-1, 1)\n",
    "        if y_pred_eval_np.ndim == 1:\n",
    "            y_pred_eval_np = y_pred_eval_np.reshape(-1, 1)\n",
    "        \n",
    "        # Inverse MinMax scaling (applied on log1p transformed data)\n",
    "        y_true_log_unscaled = target_transformer.inverse_transform(y_true_eval_np)\n",
    "        y_pred_log_unscaled = target_transformer.inverse_transform(y_pred_eval_np)\n",
    "\n",
    "        # Inverse log1p (expm1)\n",
    "        y_true_eval_np = np.expm1(y_true_log_unscaled).flatten()\n",
    "        y_pred_eval_np = np.expm1(y_pred_log_unscaled).flatten()\n",
    "        # print(f\"DEBUG: NN Reporting metrics in original scale for {model_name}\")\n",
    "    # else:\n",
    "        # print(f\"DEBUG: NN Reporting metrics in transformed scale for {model_name}\")\n",
    "\n",
    "    mae = mean_absolute_error(y_true_eval_np, y_pred_eval_np)\n",
    "    mse = mean_squared_error(y_true_eval_np, y_pred_eval_np)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true_eval_np, y_pred_eval_np)\n",
    "    \n",
    "    print(f\"--- {model_name} Performance (Original Scale if transformer provided) ---\")\n",
    "    print(f\"MAE (Primary): {mae:.4f}\")\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"R2 Score: {r2:.4f}\")\n",
    "    print(\"-----------------------------\")\n",
    "    return {'mse': mse, 'rmse': rmse, 'mae': mae, 'r2': r2}\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original full train data shape: (4494, 108)\n",
      "New X_train shape: (4089, 108), new y_train shape: (4089,)\n",
      "X_dev (validation) shape: (405, 108), y_dev (validation) shape: (405,)\n"
     ]
    }
   ],
   "source": [
    "X_train_data = train_df.drop(columns=['y'])\n",
    "y_train_data = train_df['y']\n",
    "\n",
    "X_test_data = test_df\n",
    "\n",
    "X_train_df, X_dev_df, y_train_series, y_dev_series = train_test_split(\n",
    "    X_train_data, \n",
    "    y_train_data, \n",
    "    test_size=0.09,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f\"Original full train data shape: {X_train_data.shape}\")\n",
    "print(f\"New X_train shape: {X_train_df.shape}, new y_train shape: {y_train_series.shape}\")\n",
    "print(f\"X_dev (validation) shape: {X_dev_df.shape}, y_dev (validation) shape: {y_dev_series.shape}\")\n",
    "\n",
    "X_train_np = X_train_df.values\n",
    "X_dev_np = X_dev_df.values\n",
    "\n",
    "y_train_np = y_train_series.values.reshape(-1, 1) \n",
    "y_dev_np = y_dev_series.values.reshape(-1, 1)\n",
    "\n",
    "# Convert to PyTorch Tensors\n",
    "X_train_tensor = torch.tensor(X_train_np, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_np, dtype=torch.float32)\n",
    "X_dev_tensor = torch.tensor(X_dev_np, dtype=torch.float32)\n",
    "y_dev_tensor = torch.tensor(y_dev_np, dtype=torch.float32)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "dev_dataset = TensorDataset(X_dev_tensor, y_dev_tensor)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, config, output_dim=1):\n",
    "        super(MLP, self).__init__()\n",
    "        layers_list = [] # Use layers_list instead of layers to avoid name conflict\n",
    "        current_dim = input_dim\n",
    "        \n",
    "        num_hidden_layers = config['num_hidden_layers']\n",
    "        units_per_layer = config['units_per_layer']\n",
    "        activations_per_layer = config['activations_per_layer']\n",
    "        dropout_rates = config['dropout_rates']\n",
    "\n",
    "        for i in range(num_hidden_layers):\n",
    "            layers_list.append(nn.Linear(current_dim, units_per_layer[i]))\n",
    "            if activations_per_layer[i] == 'relu':\n",
    "                layers_list.append(nn.ReLU())\n",
    "            elif activations_per_layer[i] == 'tanh':\n",
    "                layers_list.append(nn.Tanh())\n",
    "            elif activations_per_layer[i] == 'elu':\n",
    "                layers_list.append(nn.ELU())\n",
    "            \n",
    "            if dropout_rates[i] > 0:\n",
    "                layers_list.append(nn.Dropout(dropout_rates[i]))\n",
    "            current_dim = units_per_layer[i]\n",
    "            \n",
    "        layers_list.append(nn.Linear(current_dim, output_dim))\n",
    "        self.network = nn.Sequential(*layers_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "def train_epoch(model, dataloader, criterion, optimizer, device, epoch_num, num_epochs, trial_num=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_mae = 0\n",
    "    \n",
    "    desc = f\"Epoch {epoch_num+1}/{num_epochs} [Train]\"\n",
    "    if trial_num is not None:\n",
    "        desc = f\"Trial {trial_num} - \" + desc\n",
    "    progress_bar = tqdm(dataloader, desc=desc, leave=False)\n",
    "    \n",
    "    for inputs, targets in progress_bar:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "        with torch.no_grad():\n",
    "            mae = nn.L1Loss()(outputs, targets).item()\n",
    "            total_mae += mae * inputs.size(0)\n",
    "        progress_bar.set_postfix(loss=loss.item(), mae=mae)\n",
    "        \n",
    "    avg_loss = total_loss / len(dataloader.dataset)\n",
    "    avg_mae = total_mae / len(dataloader.dataset)\n",
    "    return avg_loss, avg_mae\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion, device, epoch_num, num_epochs, trial_num=None):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_mae = 0\n",
    "\n",
    "    desc = f\"Epoch {epoch_num+1}/{num_epochs} [Val]\"\n",
    "    if trial_num is not None:\n",
    "        desc = f\"Trial {trial_num} - \" + desc    \n",
    "    progress_bar = tqdm(dataloader, desc=desc, leave=False)\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in progress_bar:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            mae = nn.L1Loss()(outputs, targets).item()\n",
    "            total_mae += mae * inputs.size(0)\n",
    "            progress_bar.set_postfix(loss=loss.item(), mae=mae)\n",
    "            \n",
    "    avg_loss = total_loss / len(dataloader.dataset)\n",
    "    avg_mae = total_mae / len(dataloader.dataset)\n",
    "    return avg_loss, avg_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim_baseline = X_train_tensor.shape[1]\n",
    "baseline_config = {\n",
    "    'num_hidden_layers': 2,\n",
    "    'units_per_layer': [64, 32],\n",
    "    'activations_per_layer': ['relu', 'relu'],\n",
    "    'dropout_rates': [0.0, 0.0],\n",
    "    'optimizer_type': 'adam',\n",
    "    'learning_rate': 0.001\n",
    "}\n",
    "\n",
    "baseline_nn_model_pt = MLP(input_dim_baseline, baseline_config).to(device)\n",
    "optimizer_baseline = optim.Adam(baseline_nn_model_pt.parameters(), lr=baseline_config['learning_rate'])\n",
    "criterion_baseline = nn.L1Loss()\n",
    "scheduler_baseline = ReduceLROnPlateau(optimizer_baseline, mode='min', factor=0.2, patience=5)\n",
    "\n",
    "EPOCHS_BASELINE = 100\n",
    "PATIENCE_BASELINE = 15\n",
    "best_val_mae_baseline = float('inf')\n",
    "patience_counter_baseline = 0\n",
    "best_baseline_model_state_dict = None\n",
    "\n",
    "history_baseline_pt = {'loss': [], 'val_loss': [], PRIMARY_METRIC: [], f'val_{PRIMARY_METRIC}': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS_BASELINE):\n",
    "    train_loss, train_mae = train_epoch(baseline_nn_model_pt, train_loader, criterion_baseline, optimizer_baseline, device, epoch, EPOCHS_BASELINE)\n",
    "    val_loss, val_mae = validate_epoch(baseline_nn_model_pt, dev_loader, criterion_baseline, device, epoch, EPOCHS_BASELINE)\n",
    "    history_baseline_pt['loss'].append(train_loss); history_baseline_pt[PRIMARY_METRIC].append(train_mae)\n",
    "    history_baseline_pt['val_loss'].append(val_loss); history_baseline_pt[f'val_{PRIMARY_METRIC}'].append(val_mae)\n",
    "    print(f\"Baseline Epoch {epoch+1}/{EPOCHS_BASELINE} => Train Loss: {train_loss:.4f}, Train MAE: {train_mae:.4f} | Val Loss: {val_loss:.4f}, Val MAE: {val_mae:.4f}\")\n",
    "    scheduler_baseline.step(val_mae)\n",
    "    \n",
    "    if val_mae < best_val_mae_baseline:\n",
    "        best_val_mae_baseline = val_mae\n",
    "        patience_counter_baseline = 0\n",
    "        best_baseline_model_state_dict = copy.deepcopy(baseline_nn_model_pt.state_dict())\n",
    "        # torch.save(best_baseline_model_state_dict, 'best_baseline_nn_model_pt.pth') # Save if desired\n",
    "        print(f\"  Baseline Val MAE improved to {val_mae:.4f}.\")\n",
    "    else:\n",
    "        patience_counter_baseline += 1\n",
    "        # print(f\"  Baseline Val MAE did not improve. Patience: {patience_counter_baseline}/{PATIENCE_BASELINE}\")\n",
    "    if patience_counter_baseline >= PATIENCE_BASELINE: print(\"Baseline early stopping.\"); break\n",
    "\n",
    "print(\"\\nEvaluating Baseline PyTorch NN Model on Dev Set...\")\n",
    "if best_baseline_model_state_dict: baseline_nn_model_pt.load_state_dict(best_baseline_model_state_dict)\n",
    "baseline_nn_model_pt.eval()\n",
    "all_preds_baseline = []\n",
    "all_true_baseline = [] # For collecting y_dev_tensor parts\n",
    "with torch.no_grad():\n",
    "    for inputs, targets_batch in dev_loader: # targets_batch will be from y_dev_tensor\n",
    "        all_preds_baseline.append(baseline_nn_model_pt(inputs.to(device)).cpu())\n",
    "        all_true_baseline.append(targets_batch.cpu()) \n",
    "\n",
    "y_pred_baseline_dev_pt = torch.cat(all_preds_baseline) # Already a tensor\n",
    "y_true_baseline_dev_pt = torch.cat(all_true_baseline) # Already a tensor\n",
    "\n",
    "baseline_nn_results_pt = evaluate_nn_model(y_dev_series.values, y_pred_baseline_dev_pt, model_name=\"Baseline PyTorch NN\")\n",
    "\n",
    "pd.DataFrame(history_baseline_pt).plot(figsize=(8, 5)); plt.title(\"Baseline PyTorch NN Model Training History\")\n",
    "plt.gca().set_ylim(0, np.max(history_baseline_pt[f'val_{PRIMARY_METRIC}'])*1.2 if history_baseline_pt[f'val_{PRIMARY_METRIC}'] else None); plt.grid(True); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'num_hidden_layers': [1, 2, 3],\n",
    "    'units_options': [[32], [64], [128], [256], [64, 32], [128, 64], [256, 128], [128, 64, 32], [256, 128, 64]],\n",
    "    'activation_options': ['relu', 'tanh', 'elu'],\n",
    "    'dropout_options': [0.0, 0.1, 0.2, 0.3],\n",
    "    'learning_rate_options': [1e-4, 5e-4, 1e-3, 5e-3, 1e-2],\n",
    "    'optimizer_options': ['adam', 'rmsprop'] # NAdam can be added if preferred\n",
    "}\n",
    "\n",
    "N_RANDOM_TRIALS = 10 # Number of random configurations to try. Increase for better search.\n",
    "TUNER_EPOCHS = 30    # Max epochs for each trial during tuning. Early stopping is active.\n",
    "TUNER_PATIENCE = 7   # Patience for early stopping during tuning trials.\n",
    "\n",
    "all_trial_results = []\n",
    "best_trial_mae = float('inf')\n",
    "best_trial_config = None\n",
    "best_trial_model_state_dict = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (1667008824.py, line 66)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[27], line 66\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(f\"Best Trial Configuration Found:\u001b[0m\n\u001b[0m                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "for trial_idx in range(N_RANDOM_TRIALS):\n",
    "    print(f\"\\n--- Starting Trial {trial_idx+1}/{N_RANDOM_TRIALS} ---\")\n",
    "    \n",
    "    # Sample random configuration\n",
    "    config = {}\n",
    "    config['num_hidden_layers'] = random.choice(param_grid['num_hidden_layers'])\n",
    "    \n",
    "    # Filter units_options based on num_hidden_layers\n",
    "    possible_units = [u for u in param_grid['units_options'] if len(u) == config['num_hidden_layers']]\n",
    "    if not possible_units: # Fallback if no exact match (shouldn't happen with good grid design)\n",
    "        config['units_per_layer'] = random.choice(param_grid['units_options'])[0:config['num_hidden_layers']]\n",
    "    else:\n",
    "        config['units_per_layer'] = random.choice(possible_units)\n",
    "        \n",
    "    config['activations_per_layer'] = [random.choice(param_grid['activation_options']) for _ in range(config['num_hidden_layers'])]\n",
    "    config['dropout_rates'] = [random.choice(param_grid['dropout_options']) for _ in range(config['num_hidden_layers'])]\n",
    "    config['learning_rate'] = random.choice(param_grid['learning_rate_options'])\n",
    "    config['optimizer_type'] = random.choice(param_grid['optimizer_options'])\n",
    "\n",
    "    print(f\"Trial {trial_idx+1} Config: {config}\")\n",
    "\n",
    "    trial_model = MLP(input_dim_baseline, config).to(device)\n",
    "    if config['optimizer_type'] == 'adam':\n",
    "        trial_optimizer = optim.Adam(trial_model.parameters(), lr=config['learning_rate'])\n",
    "    elif config['optimizer_type'] == 'rmsprop':\n",
    "        trial_optimizer = optim.RMSprop(trial_model.parameters(), lr=config['learning_rate'])\n",
    "    # Add other optimizers if in options\n",
    "    \n",
    "    trial_criterion = nn.L1Loss()\n",
    "    trial_scheduler = ReduceLROnPlateau(trial_optimizer, mode='min', factor=0.2, patience=3, verbose=False)\n",
    "\n",
    "    current_best_trial_epoch_mae = float('inf')\n",
    "    current_trial_patience_counter = 0\n",
    "    current_best_trial_epoch_state_dict = None\n",
    "\n",
    "    for epoch in range(TUNER_EPOCHS):\n",
    "        train_loss, train_mae = train_epoch(trial_model, train_loader, trial_criterion, trial_optimizer, device, epoch, TUNER_EPOCHS, trial_num=trial_idx+1)\n",
    "        val_loss, val_mae = validate_epoch(trial_model, dev_loader, trial_criterion, device, epoch, TUNER_EPOCHS, trial_num=trial_idx+1)\n",
    "        # No extensive history tracking for each trial epoch to save memory, just print\n",
    "        print(f\"  Trial {trial_idx+1} - Epoch {epoch+1}/{TUNER_EPOCHS} => Train MAE: {train_mae:.4f} | Val MAE: {val_mae:.4f}\")\n",
    "        trial_scheduler.step(val_mae)\n",
    "\n",
    "        if val_mae < current_best_trial_epoch_mae:\n",
    "            current_best_trial_epoch_mae = val_mae\n",
    "            current_trial_patience_counter = 0\n",
    "            current_best_trial_epoch_state_dict = copy.deepcopy(trial_model.state_dict())\n",
    "        else:\n",
    "            current_trial_patience_counter += 1\n",
    "        \n",
    "        if current_trial_patience_counter >= TUNER_PATIENCE:\n",
    "            print(f\"  Trial {trial_idx+1} early stopping at epoch {epoch+1}.\")\n",
    "            break\n",
    "    \n",
    "    trial_result = {'config': config, 'val_mae': current_best_trial_epoch_mae, 'state_dict': current_best_trial_epoch_state_dict}\n",
    "    all_trial_results.append(trial_result)\n",
    "    print(f\"Trial {trial_idx+1} Best Val MAE: {current_best_trial_epoch_mae:.4f}\")\n",
    "\n",
    "    if current_best_trial_epoch_mae < best_trial_mae:\n",
    "        best_trial_mae = current_best_trial_epoch_mae\n",
    "        best_trial_config = config\n",
    "        best_trial_model_state_dict = current_best_trial_epoch_state_dict\n",
    "        print(f\"*** New Best Overall Trial MAE: {best_trial_mae:.4f} ***\")\n",
    "\n",
    "print(\"\\nHyperparameter Optimization (Randomized Search) Complete.\")\n",
    "if best_trial_config:\n",
    "    print(f\"Best Trial Configuration Found: {best_trial_config}\")\n",
    "    print(f\"Best Trial Validation MAE: {best_trial_mae:.4f}\")\n",
    "else:\n",
    "    print(\"No successful trials completed or no improvement found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_trial_config:\n",
    "    print(\"\\nBuilding and Training Final Best PyTorch NN Model from Search...\")\n",
    "    final_best_model_pt = MLP(input_dim_baseline, best_trial_config).to(device)\n",
    "    if best_trial_config['optimizer_type'] == 'adam':\n",
    "        final_optimizer = optim.Adam(final_best_model_pt.parameters(), lr=best_trial_config['learning_rate'])\n",
    "    elif best_trial_config['optimizer_type'] == 'rmsprop':\n",
    "        final_optimizer = optim.RMSprop(final_best_model_pt.parameters(), lr=best_trial_config['learning_rate'])\n",
    "\n",
    "    final_criterion = nn.L1Loss()\n",
    "    # Use longer patience for final model training\n",
    "    final_scheduler = ReduceLROnPlateau(final_optimizer, mode='min', factor=0.2, patience=7, verbose=True) \n",
    "\n",
    "    FINAL_MODEL_EPOCHS = 150 # Max epochs for final model\n",
    "    FINAL_MODEL_PATIENCE = 20 # Longer patience for final model\n",
    "    \n",
    "    current_best_final_model_val_mae = float('inf')\n",
    "    final_model_patience_counter = 0\n",
    "    # Load state from best trial if it makes sense, or train from scratch\n",
    "    # For simplicity here, we train from scratch using best config. Can also load best_trial_model_state_dict.\n",
    "    # if best_trial_model_state_dict: final_best_model_pt.load_state_dict(best_trial_model_state_dict)\n",
    "    \n",
    "    history_final_best_pt = {'loss': [], 'val_loss': [], PRIMARY_METRIC: [], f'val_{PRIMARY_METRIC}': []}\n",
    "\n",
    "    for epoch in range(FINAL_MODEL_EPOCHS):\n",
    "        train_loss, train_mae = train_epoch(final_best_model_pt, train_loader, final_criterion, final_optimizer, device, epoch, FINAL_MODEL_EPOCHS)\n",
    "        val_loss, val_mae = validate_epoch(final_best_model_pt, dev_loader, final_criterion, device, epoch, FINAL_MODEL_EPOCHS)\n",
    "        history_final_best_pt['loss'].append(train_loss); history_final_best_pt[PRIMARY_METRIC].append(train_mae)\n",
    "        history_final_best_pt['val_loss'].append(val_loss); history_final_best_pt[f'val_{PRIMARY_METRIC}'].append(val_mae)\n",
    "        print(f\"Final Model Epoch {epoch+1}/{FINAL_MODEL_EPOCHS} => Train MAE: {train_mae:.4f} | Val MAE: {val_mae:.4f}\")\n",
    "        final_scheduler.step(val_mae)\n",
    "        if val_mae < current_best_final_model_val_mae:\n",
    "            current_best_final_model_val_mae = val_mae\n",
    "            final_model_patience_counter = 0\n",
    "            torch.save(final_best_model_pt.state_dict(), 'final_best_nn_model_pt.pth')\n",
    "            print(f\"  Final Model Val MAE improved to {val_mae:.4f}. Saving model.\")\n",
    "        else:\n",
    "            final_model_patience_counter += 1\n",
    "        if final_model_patience_counter >= FINAL_MODEL_PATIENCE: print(\"Final model early stopping.\"); break\n",
    "\n",
    "    print(\"\\nEvaluating Final Best PyTorch NN Model on Dev Set...\")\n",
    "    final_best_model_pt.load_state_dict(torch.load('final_best_nn_model_pt.pth')) # Load best saved state\n",
    "    final_best_model_pt.eval()\n",
    "    all_preds_final_best = []\n",
    "    all_true_final_best = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets_batch in dev_loader: \n",
    "            all_preds_final_best.append(final_best_model_pt(inputs.to(device)).cpu())\n",
    "            all_true_final_best.append(targets_batch.cpu()) # Store the corresponding true values\n",
    "    \n",
    "    y_pred_final_best_dev_pt = torch.cat(all_preds_final_best) # Already a tensor\n",
    "    y_true_final_best_dev_pt = torch.cat(all_true_final_best) # Already a tensor\n",
    "    \n",
    "    final_best_nn_results_pt = evaluate_nn_model(y_dev_series.values, y_pred_final_best_dev_pt, model_name=\"Final Best PyTorch NN\")\n",
    "\n",
    "    pd.DataFrame(history_final_best_pt).plot(figsize=(8, 5)); plt.title(\"Final Best PyTorch NN Model Training History (MAE on Transformed Target)\")\n",
    "    plt.gca().set_ylim(0, np.max(history_final_best_pt[f'val_{PRIMARY_METRIC}'])*1.2 if history_final_best_pt[f'val_{PRIMARY_METRIC}'] else None); plt.grid(True); plt.show()\n",
    "else:\n",
    "    print(\"Skipping final model training as no best trial configuration was found.\")\n",
    "    final_best_nn_results_pt = None # Ensure variable exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nBaseline PyTorch NN Model (on Dev Set):\")\n",
    "if 'baseline_nn_results_pt' in locals() and baseline_nn_results_pt:\n",
    "    print(f\"  MAE: {baseline_nn_results_pt['mae']:.4f}, MSE: {baseline_nn_results_pt['mse']:.4f}, R2: {baseline_nn_results_pt['r2']:.4f}\")\n",
    "else:\n",
    "    print(\"  Baseline model results not available.\")\n",
    "\n",
    "print(\"\\nFinal Best PyTorch NN Model from Randomized Search (on Dev Set):\")\n",
    "if final_best_nn_results_pt: # Check if it was set\n",
    "    print(f\"  MAE: {final_best_nn_results_pt['mae']:.4f}, MSE: {final_best_nn_results_pt['mse']:.4f}, R2: {final_best_nn_results_pt['r2']:.4f}\")\n",
    "else:\n",
    "    print(\"  Final best model results not available (e.g., search yielded no improvements or was skipped).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory if it doesn't exist\n",
    "models_dir = 'models'\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "    print(f\"Created directory: {models_dir}\")\n",
    "\n",
    "# Save baseline model if it exists\n",
    "if 'baseline_nn_model_pt' in locals() and baseline_nn_model_pt is not None:\n",
    "    baseline_path = os.path.join(models_dir, 'baseline_nn_model.pt')\n",
    "    torch.save({\n",
    "        'model_state_dict': baseline_nn_model_pt.state_dict(),\n",
    "        'config': baseline_config,\n",
    "        'performance': baseline_nn_results_pt if 'baseline_nn_results_pt' in locals() else None\n",
    "    }, baseline_path)\n",
    "    print(f\"Saved baseline model to: {baseline_path}\")\n",
    "\n",
    "# Save final best model if it exists\n",
    "if 'final_best_model_pt' in locals() and final_best_model_pt is not None:\n",
    "    final_model_path = os.path.join(models_dir, 'final_best_nn_model.pt')\n",
    "    torch.save({\n",
    "        'model_state_dict': final_best_model_pt.state_dict(),\n",
    "        'config': best_trial_config,\n",
    "        'performance': final_best_nn_results_pt if 'final_best_nn_results_pt' in locals() else None\n",
    "    }, final_model_path)\n",
    "    print(f\"Saved final best model to: {final_model_path}\")\n",
    "\n",
    "# Save hyperparameter search results\n",
    "if 'all_trial_results' in locals() and all_trial_results:\n",
    "    # Convert trial results to serializable format\n",
    "    serializable_trials = []\n",
    "    for trial in all_trial_results:\n",
    "        serializable_trial = {\n",
    "            'config': trial['config'],\n",
    "            'val_mae': float(trial['val_mae'])  # Convert to Python float\n",
    "        }\n",
    "        serializable_trials.append(serializable_trial)\n",
    "    \n",
    "    trials_path = os.path.join(models_dir, 'hyperparameter_trials.json')\n",
    "    with open(trials_path, 'w') as f:\n",
    "        json.dump(serializable_trials, f, indent=2)\n",
    "    print(f\"Saved hyperparameter trials to: {trials_path}\")\n",
    "\n",
    "# Save best trial configuration if it exists\n",
    "if 'best_trial_config' in locals() and best_trial_config is not None:\n",
    "    best_config_path = os.path.join(models_dir, 'best_nn_config.json')\n",
    "    with open(best_config_path, 'w') as f:\n",
    "        json.dump(best_trial_config, f, indent=2)\n",
    "    print(f\"Saved best configuration to: {best_config_path}\")\n",
    "\n",
    "# Save feature scaler\n",
    "if 'feature_scaler' in locals() and feature_scaler is not None:\n",
    "    scaler_path = os.path.join(models_dir, 'feature_scaler.joblib')\n",
    "    joblib.dump(feature_scaler, scaler_path)\n",
    "    print(f\"Saved feature scaler to: {scaler_path}\")\n",
    "\n",
    "# Save target transformer\n",
    "if 'target_transformer_pt' in locals() and target_transformer_pt is not None:\n",
    "    target_transformer_path = os.path.join(models_dir, 'target_transformer.joblib')\n",
    "    joblib.dump(target_transformer_pt, target_transformer_path)\n",
    "    print(f\"Saved target transformer to: {target_transformer_path}\")\n",
    "\n",
    "print(\"\\nAll models and parameters have been saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For more robust hyperparameter optimization in PyTorch, use dedicated libraries like Optuna or Ray Tune.\n",
    "   - The randomized search here is a simplified simulation.\n",
    "\n",
    "2. After finding a promising configuration, consider a more focused 'GridSearch-like' manual tuning around those best parameters.\n",
    "\n",
    "3. Retrain the absolute best model on the *entire* X_full_train_df and y_full_train_series for a production model.\n",
    "   - Remember to scale X_full_train_df appropriately\n",
    "   - Evaluate on a true unseen TEST set if available\n",
    "\n",
    "4. Experiment with batch_size as another hyperparameter to tune.\n",
    "\n",
    "5. If categorical features are significant, consider using nn.Embedding layers.\n",
    "\n",
    "6. Analyze error patterns (e.g., residuals plot) for the best model on the dev set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".rent_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
